{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing http://newyorksocialdiary.com/party-pictures?page=1\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=2\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=3\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=4\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=5\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=6\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=7\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=8\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=9\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=10\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=11\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=12\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=13\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=14\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=15\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=16\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=17\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=18\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=19\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=20\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=21\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=22\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=23\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=24\n",
      "Parsing http://newyorksocialdiary.com/party-pictures?page=25\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=1\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=2\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=3\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=4\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=5\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=6\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=7\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=8\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=9\n",
      "503\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=10\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=11\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=12\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=13\n",
      "503\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=14\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=15\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=16\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=17\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=18\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=19\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=20\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=21\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=22\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=23\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=24\n",
      "Parsing http://www.newyorksocialdiary.com/party-pictures?page=25\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "def checkDate(partyDate,datecheck):\n",
    "    nycDate = partyDate.find('span',attrs={'class': 'views-field views-field-created'}).text.strip()\n",
    "    return datetime.strptime(nycDate,'%A, %B %d, %Y') < datecheck\n",
    "\n",
    "def partyList(urlroot,pageNum):\n",
    "    partylinklist = [] \n",
    "    url = urlroot+'/party-pictures?page='+str(pageNum)\n",
    "    print 'Parsing '+url\n",
    "    try:\n",
    "        partyPage = urllib2.urlopen(url).read()\n",
    "    except urllib2.HTTPError as e:\n",
    "        print e.code\n",
    "        time.sleep(30)\n",
    "        partyPage = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(partyPage)\n",
    "    partylist = soup.find('div',attrs={'class': 'view-content'})\n",
    "    parties = partylist.find_all('div',attrs={'class': 'views-row'})\n",
    "    for p in parties:\n",
    "        if checkDate(p,datetime(2014, 12, 1, 0,0)):\n",
    "          partyLink = urlroot+p.find('a').attrs['href']\n",
    "          if partyLink != 'http://www.newyorksocialdiary.com/nysd/partypictures':\n",
    "              partylinklist.append(partyLink)\n",
    "    return partylinklist\n",
    "\n",
    "for page in np.arange(1,26):\n",
    "    parties = partyList(\"http://newyorksocialdiary.com\",page)\n",
    "    \n",
    "def getCaptions(purl):\n",
    "    caplist=[];\n",
    "    try:\n",
    "        page = urllib2.urlopen(purl).read()\n",
    "    except urllib2.HTTPError as e:\n",
    "        print e.code\n",
    "        time.sleep(30)\n",
    "        page = urllib2.urlopen(purl).read()\n",
    "    findCaption = BeautifulSoup(page)\n",
    "    findDivCaption = findCaption.find_all('div', attrs={'class': 'photocaption'})\n",
    "    findIdCap = findCaption.find_all('div', attrs={'id': 'photocaption'})\n",
    "    findClassCaptions = findCaption.find_all('td', attrs={'class': 'photocaption'})\n",
    "    findByFont = findCaption.find_all('font', attrs={'face': 'Verdana, Arial, Helvetica, sans-serif','size': '1'})\n",
    "    nycCaptions=findDivCaption+findIdCap+findClassCaptions+findByFont    \n",
    "    for c in nycCaptions:\n",
    "        caplist.append(re.sub(r\"(\\s\\s)+\",' ',c.text).strip())\n",
    "    return list(set(caplist))\n",
    "        \n",
    "def scapeNyds():\n",
    "    urlroot = \"http://www.newyorksocialdiary.com\"\n",
    "    for page in np.arange(1,26):\n",
    "        captions = []\n",
    "        parties = partyList(urlroot,page)\n",
    "        for p in parties:\n",
    "            captions = captions+getCaptions(p)\n",
    "        fileName = 'partypage%02i' % page\n",
    "        f = codecs.open(fileName,mode='wb',encoding='utf-8')\n",
    "        for c in captions:\n",
    "             f.write(c+'\\n')\n",
    "        f.close()     \n",
    "\n",
    "scapeNyds()  \n",
    "\n",
    "def loadPages():\n",
    "    newcaps = []\n",
    "    for page in np.arange(1,26):\n",
    "        f = codecs.open('partypage%02i'%page, encoding='utf-8')\n",
    "        for line in f:\n",
    "            newcaps.append(re.sub(r'\\n','',line))\n",
    "        f.close()\n",
    "    return newcaps\n",
    "\n",
    "def removeExcessCaptions(caps):\n",
    "    for c in caps:\n",
    "        if len(c) > 250:\n",
    "            caps.remove(c)\n",
    "        elif re.match('Photo',c):\n",
    "            caps.remove(c)\n",
    "        elif re.match('Click',c):\n",
    "            caps.remove(c)\n",
    "        elif c =='':\n",
    "            caps.remove(c)\n",
    "\n",
    "def cleanCaptions(c):\n",
    "    c = re.sub('\\s+', ' ', c)\n",
    "    c = re.sub('\\(.+\\)','',c)\n",
    "    c = re.sub('\\[.+\\]','',c)\n",
    "    c = re.sub('\\.','',c)\n",
    "    c = re.sub(r\"(Philanthro\\w+ |Trustees |Sundance |Host |Co-Chairm[ea]n |Board |Mr |Mrs |Founder |founder |Smithsonian |Secretary |Chairs |Gala |Baroness |Baron |Mayor |Co-chairs |Honorable |Vice |Diretor |director |Principal |Museum |Magician |Senior |Lady |Sir |Event |Exec |exec |Prince* |King |Chairman |Chairmen |honoree |Honoree |Governor |Commissioner |Trustee | trustee |President |Speaker |Dr |Doctor |Ambassador |Senator |Excellency |Executive |executive |Committee |committee |Chair |chair |Associate |[Mm]ember[s]* |[A-H|J-Z]{2,})\",'',c)\n",
    "    c = re.sub('\\s+with\\s+([A-Z][a-z]+)\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)$',r',\\1 \\3,\\2 \\3',c)  \n",
    "    c = re.sub('\\s+and\\s+([A-Z][a-z]+)\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)$',r',\\1 \\3,\\2 \\3',c)\n",
    "    c = re.sub('^([A-Z][a-z]+),\\s+([A-Z][a-z]+),\\s+([A-Z][a-z]+),\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)',r'\\1 \\5,\\2 \\5,\\3 \\5,\\4 \\5',c)\n",
    "    c = re.sub('^([A-Z][a-z]+),\\s+([A-Z][a-z]+),\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)',r'\\1 \\4,\\2 \\4,\\3 \\4',c)\n",
    "    c = re.sub('^([A-Z][a-z]+)\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)',r'\\1 \\3,\\2 \\3',c)\n",
    "    c = re.sub('[,:;.]\\s+([A-Z][a-z]+)\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)',r',\\1 \\3,\\2 \\3,',c)\n",
    "    c = re.sub('^([A-Z][a-z]+)\\s+([A-Z][a-z]+)\\s+with\\s+([A-Z][a-z]+)\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)',r'\\1 \\2,\\3 \\5,\\4 \\5',c)    \n",
    "    c = re.sub('[,:;.]\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)\\s+with\\s+([A-Z][a-z]+)\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)',r'\\1 \\2,\\3 \\5,\\4 \\5',c)     \n",
    "    c = re.sub('^([A-Z][a-z]+)\\s+([A-Z][a-z]+)\\s+with\\s+([A-Z][a-z]+)',r'\\1 \\2,\\3 \\2',c)\n",
    "    c = re.sub('[,:;.]\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)\\s+with\\s+([A-Z][a-z]+)',r'\\1 \\2,\\3 \\2',c)   \n",
    "    c = re.sub('\\s+and\\s+',',',c)\n",
    "    c = re.sub('\\s+with\\s+',',',c)\n",
    "    c = re.sub('\\s*,\\s+',',',c)\n",
    "    c = re.sub(',,',',',c)\n",
    "    c = re.sub('^[\\s+|,]','',c)\n",
    "    c = re.sub(',and\\s+',',',c)\n",
    "    c = re.sub('([\\s,][Hh]is |[\\s,][Hh]er |[\\s,][Tt]heir |[\\s,][Tt]he )',' ',c)\n",
    "    return c\n",
    "            \n",
    "capList = loadPages()\n",
    "removeExcessCaptions(capList)\n",
    "removeCapList = [cleanCaptions(x) for x in capList]\n",
    "len(set(removeCapList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count is : 75947\n"
     ]
    }
   ],
   "source": [
    "v = len(set(namesOf))\n",
    "print \"Total count is : \" + str(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
